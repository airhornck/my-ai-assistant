from __future__ import annotations

import json
import logging
import time
from typing import Any, TypedDict

from langgraph.graph import END, StateGraph

from models.request import ContentRequest
from domain.memory import MemoryService
from services.ai_service import SimpleAIService
from workflows.evaluation_node import create_evaluation_node

logger = logging.getLogger(__name__)


class State(TypedDict):
    """
    å·¥ä½œæµçŠ¶æ€å®šä¹‰ - ä½¿ç”¨ TypedDict ç¡®ä¿ç±»å‹å®‰å…¨ã€‚
    """
    user_input: str
    analysis: str
    content: str
    session_id: str
    user_id: str
    evaluation: dict
    need_revision: bool
    stage_durations: dict  # å„é˜¶æ®µè€—æ—¶ï¼ˆç§’ï¼‰ï¼Œå¦‚ {"analyze": 0.5, "generate": 1.2, ...}
    analyze_cache_hit: bool  # åˆ†æé˜¶æ®µæ˜¯å¦å‘½ä¸­ç¼“å­˜
    used_tags: list  # æœ¬æ¬¡å®é™…ä¼ ç»™æ¨¡å‹çš„æ ‡ç­¾ï¼ˆè¯·æ±‚è¾“å…¥è¦†ç›–æˆ–ç³»ç»Ÿå†å²ç”Ÿæˆï¼Œä¾›å“åº”è¿”å›ï¼‰


def create_workflow(ai_service: SimpleAIService | None = None) -> Any:
    """
    åˆ›å»ºå·¥ä½œæµå›¾ã€‚å¯æ³¨å…¥ ai_serviceï¼ˆå¦‚å¸¦ç¼“å­˜çš„å®ä¾‹ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨æ–°å»ºçš„ SimpleAIService()ã€‚
    èŠ‚ç‚¹ä¼šè®°å½•å„é˜¶æ®µè€—æ—¶ï¼ˆstage_durationsï¼‰åŠåˆ†æé˜¶æ®µæ˜¯å¦å‘½ä¸­ç¼“å­˜ï¼ˆanalyze_cache_hitï¼‰ã€‚
    """
    ai_svc = ai_service or SimpleAIService()
    memory_svc = MemoryService()

    async def _analyze_node(state: State) -> State:
        t0 = time.perf_counter()
        user_id = state.get("user_id") or ""
        try:
            data = json.loads(state["user_input"])
            request = ContentRequest(**data)
        except (json.JSONDecodeError, TypeError):
            request = ContentRequest(
                user_id=user_id,
                brand_name="",
                product_desc=state.get("user_input", ""),
                topic="",
            )
        tags_override = list(request.tags) if (getattr(request, "tags", None) and len(request.tags) > 0) else None
        try:
            memory = await memory_svc.get_memory_for_analyze(
                user_id=user_id,
                brand_name=request.brand_name or "",
                product_desc=request.product_desc or "",
                topic=request.topic or "",
                tags_override=tags_override,
            )
            preference_context = memory.get("preference_context", "") or None
            context_fingerprint = memory.get("context_fingerprint") or {"tags": [], "recent_topics": []}
            effective_tags = memory.get("effective_tags") or []
        except Exception as e:
            logger.warning("analyze_node MemoryService æŸ¥è¯¢å¤±è´¥ï¼Œé™çº§ä¸ºç©ºä¸Šä¸‹æ–‡: %s", e, exc_info=True)
            preference_context = None
            context_fingerprint = {"tags": sorted(str(t) for t in (tags_override or [])), "recent_topics": []}
            effective_tags = tags_override or []
        analysis_result, cache_hit = await ai_svc.analyze(
            request,
            preference_context=preference_context or None,
            context_fingerprint=context_fingerprint,
        )
        duration = round(time.perf_counter() - t0, 4)
        return {
            **state,
            "analysis": analysis_result,
            "evaluation": state.get("evaluation", {}),
            "need_revision": state.get("need_revision", False),
            "stage_durations": {**state.get("stage_durations", {}), "analyze": duration},
            "analyze_cache_hit": cache_hit,
            "used_tags": effective_tags,
        }

    async def _generate_node(state: State) -> State:
        t0 = time.perf_counter()
        topic, raw_query, doc_ctx = "", "", ""
        try:
            ui = state.get("user_input", "")
            data = json.loads(ui) if isinstance(ui, str) else {}
            if isinstance(data, dict):
                topic = str(data.get("topic", "") or "")
                raw_query = str(data.get("raw_query", "") or "")
                doc_ctx = str(data.get("session_document_context", "") or "")
        except (json.JSONDecodeError, TypeError):
            pass
        generated_content = await ai_svc.generate(
            state["analysis"],
            topic=topic,
            raw_query=raw_query,
            session_document_context=doc_ctx,
        )
        duration = round(time.perf_counter() - t0, 4)
        return {
            **state,
            "content": generated_content,
            "evaluation": state.get("evaluation", {}),
            "need_revision": state.get("need_revision", False),
            "stage_durations": {**state.get("stage_durations", {}), "generate": duration},
            "analyze_cache_hit": state.get("analyze_cache_hit", False),
            "used_tags": state.get("used_tags", []),
        }

    def _format_node(state: State) -> State:
        t0 = time.perf_counter()
        analysis = state.get("analysis")
        if isinstance(analysis, dict):
            analysis_display = (
                f"å¾—åˆ† {analysis.get('semantic_score', 0)}ï¼›"
                f"åˆ‡å…¥ç‚¹ï¼š{analysis.get('angle', '')}ï¼›ç†ç”±ï¼š{analysis.get('reason', '')}"
            )
        else:
            analysis_display = analysis if isinstance(analysis, str) else ""
        formatted_content = f"ğŸ“ æ¨å¹¿æ–‡æ¡ˆï¼š\n\n{state['content']}\n\nâœ¨ åŸºäºåˆ†æï¼š{analysis_display}"
        duration = round(time.perf_counter() - t0, 4)
        return {
            **state,
            "content": formatted_content,
            "evaluation": state.get("evaluation", {}),
            "need_revision": state.get("need_revision", False),
            "stage_durations": {**state.get("stage_durations", {}), "format": duration},
            "analyze_cache_hit": state.get("analyze_cache_hit", False),
            "used_tags": state.get("used_tags", []),
        }

    workflow = StateGraph(State)
    workflow.add_node("analyze", _analyze_node)
    workflow.add_node("generate", _generate_node)
    workflow.add_node("format", _format_node)
    workflow.add_node("evaluate", create_evaluation_node(ai_svc))
    workflow.set_entry_point("analyze")
    workflow.add_edge("analyze", "generate")
    workflow.add_edge("generate", "format")
    workflow.add_edge("format", "evaluate")
    workflow.add_edge("evaluate", END)
    return workflow.compile()